# Novel friction-adaptive descent algorithm and its potential applications in machine learning and non-convex optimization
## Numerical experiments
Dissertation files on Kinetic Friction-Adaptive Descent (KFAD)
Kinetic Friction-Adaptive Descent (KFAD) is a novel optimization descent algorithm. The dissertation builds upon the very little research on the topic, and delves into the potential of this method in comparison to more well-established ones, such as Stochastic Gradient Descent. The two main focal points of the paper are to show the viability of KFAD as an optimization method in small neural networks and, most importantly, to demonstrate its effectiveness in the context of 2D non-convex function optimization. The findings of this paper present KFAD as a promising method. However, KFAD requires careful study to overcome the limitations of the tedious search needed for optimal hyperparameter selection.
This GitHub repository contains the files used to carry out the numerical experiments presented in the dissertation.
